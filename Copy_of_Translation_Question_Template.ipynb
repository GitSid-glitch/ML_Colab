{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GitSid-glitch/ML_Colab/blob/main/Copy_of_Translation_Question_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acb9ff16",
      "metadata": {
        "id": "acb9ff16"
      },
      "source": [
        "## 1. Import Libraries\n",
        "\n",
        "* **Import Libraries:** This cell imports the necessary libraries for data processing and neural network building.\n",
        "\n",
        "**Hints:**\n",
        "- Import `pandas` for data manipulation\n",
        "- Import `torch` and `torch.nn` for neural networks\n",
        "- Import `random` and `time` for shuffling and timing\n",
        "\n",
        "**What each function does:**\n",
        "| Function | Description |\n",
        "|----------|-------------|\n",
        "| `random.seed(42)` | Sets Python's random number generator to a fixed state for reproducible results |\n",
        "| `torch.manual_seed(42)` | Sets PyTorch's random seed so neural network weights initialize the same way every run |\n",
        "\n",
        "**Documentation:**\n",
        "- [pandas](https://pandas.pydata.org/docs/) - Data analysis library\n",
        "- [torch](https://pytorch.org/docs/stable/torch.html) - PyTorch tensor operations\n",
        "- [torch.nn](https://pytorch.org/docs/stable/nn.html) - Neural network building blocks\n",
        "- [random.seed](https://docs.python.org/3/library/random.html#random.seed) - Initialize random generator\n",
        "- [torch.manual_seed](https://pytorch.org/docs/stable/generated/torch.manual_seed.html) - Set PyTorch random seed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies as needed:\n",
        "# pip install kagglehub[pandas-datasets]\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Set the path to the file you'd like to load\n",
        "file_path = \"Dataset_English_Hindi.csv\"\n",
        "\n",
        "# Load the latest version\n",
        "df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"preetviradiya/english-hindi-dataset\",\n",
        "  file_path,\n",
        "  # Provide any additional arguments like\n",
        "  # sql_query or pandas_kwargs. See the\n",
        "  # documenation for more information:\n",
        "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
        ")\n",
        "\n",
        "print(\"First 5 records:\", df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJdn_-aSIaUb",
        "outputId": "090aaf9e-5039-42d8-f4aa-d9cc956eeeff"
      },
      "id": "wJdn_-aSIaUb",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1790220126.py:10: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'english-hindi-dataset' dataset.\n",
            "First 5 records:   English    Hindi\n",
            "0   Help!    बचाओ!\n",
            "1   Jump.    उछलो.\n",
            "2   Jump.    कूदो.\n",
            "3   Jump.   छलांग.\n",
            "4  Hello!  नमस्ते।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2d40549",
      "metadata": {
        "id": "b2d40549"
      },
      "source": [
        "# Dataset URL\n",
        "https://www.kaggle.com/datasets/preetviradiya/english-hindi-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a0c0ec70",
      "metadata": {
        "id": "a0c0ec70",
        "outputId": "ec67540f-fa04-4bc3-848a-ecf8f7470e1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries loaded!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch  # Hint: PyTorch deep learning library\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)  # Hint: Function to set PyTorch's random seed\n",
        "\n",
        "print(\"Libraries loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95ad38b0",
      "metadata": {
        "id": "95ad38b0"
      },
      "source": [
        "## 2. Load Data\n",
        "\n",
        "* **Load Dataset:** This cell loads the English-Hindi translation dataset from a CSV file and displays sample translations.\n",
        "\n",
        "**Hints:**\n",
        "- Use `pd.read_csv()` to load CSV files\n",
        "- Use `len(df)` to get number of rows\n",
        "- Access columns with `df['column'][index]`\n",
        "\n",
        "**What each function does:**\n",
        "| Function | Description |\n",
        "|----------|-------------|\n",
        "| `pd.read_csv('file.csv')` | Reads a CSV file and returns a DataFrame (table) with rows and columns |\n",
        "| `len(df)` | Returns the total number of rows in the DataFrame |\n",
        "| `df['column'][i]` | Accesses the value at row `i` of the specified column |\n",
        "\n",
        "**Documentation:**\n",
        "- [pd.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) - Read CSV into DataFrame\n",
        "- [DataFrame indexing](https://pandas.pydata.org/docs/user_guide/indexing.html) - Access DataFrame elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b20d774c",
      "metadata": {
        "id": "b20d774c",
        "outputId": "db28e250-0371-43f9-9265-9fe270d61b6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences: 130476\n",
            "\n",
            "First 5 examples:\n",
            "  Help!              → बचाओ!\n",
            "  Jump.              → उछलो.\n",
            "  Jump.              → कूदो.\n",
            "  Jump.              → छलांग.\n",
            "  Hello!             → नमस्ते।\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "# df = pd.read_csv('Dataset_English_Hindi.csv')  # Hint: Function to read CSV files\n",
        "print(f\"Total sentences: {len(df)}\")\n",
        "print(\"\\nFirst 5 examples:\")\n",
        "for i in range(5):\n",
        "    print(f\"  {df['English'][i]:18} → {df['Hindi'][i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a79701c",
      "metadata": {
        "id": "2a79701c"
      },
      "source": [
        "## 3. Prepare Data Subset\n",
        "\n",
        "* **Filter Data:** This cell filters the dataset to select short sentences and removes rows with missing data.\n",
        "\n",
        "**Hints:**\n",
        "- Use `df[condition]` for boolean indexing\n",
        "- Use `.str.len()` to get string lengths\n",
        "- Use `.head(n)` to select first n rows\n",
        "- Use `.dropna()` to remove NaN values\n",
        "- Use `.reset_index(drop=True)` to reset indices\n",
        "\n",
        "**What each function does:**\n",
        "| Function | Description |\n",
        "|----------|-------------|\n",
        "| `df['col'].str.len()` | Returns the character length of each string in the column |\n",
        "| `df[condition]` | Filters rows where condition is True (boolean indexing) |\n",
        "| `df.head(n)` | Returns first n rows of the DataFrame |\n",
        "| `df.dropna(subset=['cols'])` | Removes rows that have NaN (missing) values in specified columns |\n",
        "| `df.reset_index(drop=True)` | Resets row indices to 0,1,2,... and drops old index |\n",
        "\n",
        "**Documentation:**\n",
        "- [DataFrame.str.len](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.len.html) - String length\n",
        "- [DataFrame.head](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html) - First n rows\n",
        "- [DataFrame.dropna](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html) - Remove missing values\n",
        "- [DataFrame.reset_index](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html) - Reset row numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5d51881d",
      "metadata": {
        "id": "5d51881d",
        "outputId": "e03c799c-8ceb-4b6a-c310-ae8e738ce67a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected 1000 sentences for training\n"
          ]
        }
      ],
      "source": [
        "# Use 500 short sentences for faster training\n",
        "df_small = df[df['English'].str.len() < 30].head(1000).reset_index(drop=True)  # Hint: Get first n rows\n",
        "\n",
        "# Remove rows with missing values\n",
        "df_small = df_small.dropna(subset=['English', 'Hindi']).reset_index(drop=True)  # Hint: Remove NaN rows\n",
        "\n",
        "print(f\"Selected {len(df_small)} sentences for training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5d31c8a",
      "metadata": {
        "id": "c5d31c8a"
      },
      "source": [
        "## 5. Helper Function: Text to Numbers\n",
        "\n",
        "* **Convert Text to Indices:** This cell defines a function to convert sentences into sequences of numbers.\n",
        "\n",
        "**Hints:**\n",
        "- Use list comprehension with `dict.get()` for word lookup\n",
        "- Append `<EOS>` token (index 2) at the end\n",
        "- Use `torch.tensor()` to create PyTorch tensor\n",
        "\n",
        "**What each function does:**\n",
        "| Function | Description |\n",
        "|----------|-------------|\n",
        "| `word2idx.get(word, 3)` | Looks up word's index, returns 3 (UNK) if word not found |\n",
        "| `list.append(item)` | Adds item to end of list |\n",
        "| `torch.tensor(data, dtype)` | Creates a PyTorch tensor from Python list |\n",
        "| `tensor.tolist()` | Converts PyTorch tensor back to Python list |\n",
        "\n",
        "**Documentation:**\n",
        "- [dict.get](https://docs.python.org/3/library/stdtypes.html#dict.get) - Safe dictionary lookup\n",
        "- [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html) - Create tensor\n",
        "- [Tensor.tolist](https://pytorch.org/docs/stable/generated/torch.Tensor.tolist.html) - Tensor to list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b8586d95",
      "metadata": {
        "id": "b8586d95",
        "outputId": "d407a468-29c3-4f70-f74b-9f9ffd88c728",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English vocabulary: 489 words\n",
            "Hindi vocabulary: 535 words\n"
          ]
        }
      ],
      "source": [
        "def build_vocab(sentences, min_freq=2):\n",
        "    \"\"\"Build word to index mapping\"\"\"\n",
        "    word2idx = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "\n",
        "    # Count words\n",
        "    word_counts = {}\n",
        "    for sentence in sentences:\n",
        "        # Convert to string and skip if empty or NaN\n",
        "        sentence_str = str(sentence) if pd.notna(sentence) else \"\"\n",
        "        if sentence_str and sentence_str != 'nan':\n",
        "            for word in sentence_str.lower().split():  # Hint: Convert to lowercase\n",
        "                word_counts[word] = word_counts.get(word, 0) + 1  # Hint: Safe dict lookup with default\n",
        "\n",
        "    # Add frequent words\n",
        "    idx = 4\n",
        "    for word, count in word_counts.items():  # Hint: Get key-value pairs from dict\n",
        "        if count >= min_freq:\n",
        "            word2idx[word] = idx\n",
        "            idx += 1\n",
        "\n",
        "    idx2word = {v: k for k, v in word2idx.items()}\n",
        "    return word2idx, idx2word\n",
        "\n",
        "# Build vocabularies\n",
        "eng_word2idx, eng_idx2word = build_vocab(df_small['English'].tolist())\n",
        "hin_word2idx, hin_idx2word = build_vocab(df_small['Hindi'].tolist())\n",
        "\n",
        "print(f\"English vocabulary: {len(eng_word2idx)} words\")\n",
        "print(f\"Hindi vocabulary: {len(hin_word2idx)} words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67108230",
      "metadata": {
        "id": "67108230"
      },
      "source": [
        "## 4. Build Vocabulary\n",
        "\n",
        "* **Create Vocabulary:** This cell creates word-to-index mappings for both English and Hindi.\n",
        "\n",
        "**Hints:**\n",
        "- Initialize with special tokens: `<PAD>=0, <SOS>=1, <EOS>=2, <UNK>=3`\n",
        "- Use `dict.get(key, default)` for safe dictionary access\n",
        "- Use `pd.notna()` to check for NaN values\n",
        "- Use `str.lower().split()` to tokenize sentences\n",
        "\n",
        "**What each function does:**\n",
        "| Function | Description |\n",
        "|----------|-------------|\n",
        "| `dict.get(key, default)` | Returns value for key if exists, otherwise returns default value |\n",
        "| `pd.notna(value)` | Returns True if value is NOT missing (not NaN) |\n",
        "| `str.lower()` | Converts string to lowercase (\"Hello\" → \"hello\") |\n",
        "| `str.split()` | Splits string into list of words by whitespace |\n",
        "| `dict.items()` | Returns all key-value pairs from dictionary |\n",
        "\n",
        "**Documentation:**\n",
        "- [dict.get](https://docs.python.org/3/library/stdtypes.html#dict.get) - Safe dictionary access\n",
        "- [pd.notna](https://pandas.pydata.org/docs/reference/api/pandas.notna.html) - Check for non-null\n",
        "- [str.lower](https://docs.python.org/3/library/stdtypes.html#str.lower) - Lowercase conversion\n",
        "- [str.split](https://docs.python.org/3/library/stdtypes.html#str.split) - Split into words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "973c9ffe",
      "metadata": {
        "id": "973c9ffe",
        "outputId": "d07b57d2-034d-4cdc-82b5-bcfbde2f33ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Help!' → [3, 2]\n"
          ]
        }
      ],
      "source": [
        "def sentence_to_indices(sentence, word2idx):\n",
        "    \"\"\"Convert sentence to list of indices\"\"\"\n",
        "    indices = [word2idx.get(word.lower(), 3) for word in sentence.split()]  # Hint: Safe dict lookup\n",
        "    indices.append(2)  # Hint: Add EOS token to end of list\n",
        "    return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "# Example\n",
        "example = df_small['English'][0]\n",
        "indices = sentence_to_indices(example, eng_word2idx)\n",
        "print(f\"'{example}' → {indices.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26324896",
      "metadata": {
        "id": "26324896"
      },
      "source": [
        "## 6. Build Encoder\n",
        "\n",
        "* **Define Encoder:** This cell creates the Encoder neural network that reads English sentences and produces a context vector.\n",
        "\n",
        "**Hints:**\n",
        "- Inherit from `nn.Module`\n",
        "- Use `nn.Embedding(vocab_size, embed_size)` for word embeddings\n",
        "- Use `nn.GRU(embed_size, hidden_size, batch_first=True)` for sequence processing\n",
        "- Return the final hidden state as context\n",
        "\n",
        "**What each function does:**\n",
        "| Function | Description |\n",
        "|----------|-------------|\n",
        "| `nn.Module` | Base class for all neural networks - provides training/saving functionality |\n",
        "| `super().__init__()` | Initializes the parent nn.Module class |\n",
        "| `nn.Embedding(vocab_size, embed_size)` | Creates a lookup table that converts word indices to dense vectors |\n",
        "| `nn.GRU(input, hidden, batch_first)` | Gated Recurrent Unit - processes sequences and remembers context |\n",
        "| `self.embedding(x)` | Converts input indices to embedding vectors |\n",
        "| `self.gru(embedded)` | Processes sequence, returns (outputs, final_hidden_state) |\n",
        "\n",
        "**Documentation:**\n",
        "- [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) - Base neural network class\n",
        "- [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) - Word to vector lookup\n",
        "- [nn.GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html) - Recurrent layer for sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e4fbe7f9",
      "metadata": {
        "id": "e4fbe7f9",
        "outputId": "7d80c2e1-bab4-4a92-942a-411ef4ddc9a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder defined\n"
          ]
        }
      ],
      "source": [
        "class Encoder(nn.Module):  # Hint: Base class for PyTorch neural networks\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)  # Hint: Converts word indices to vectors\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        _, hidden = self.gru(embedded)\n",
        "        return hidden\n",
        "\n",
        "print(\"Encoder defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6649a8d",
      "metadata": {
        "id": "b6649a8d"
      },
      "source": [
        "## 7. Build Decoder\n",
        "\n",
        "* **Define Decoder:** This cell creates the Decoder neural network that generates Hindi words one at a time using the context.\n",
        "\n",
        "**Hints:**\n",
        "- Use `nn.Embedding` for Hindi word embeddings\n",
        "- Use `nn.GRU` to process with hidden state\n",
        "- Use `nn.Linear(hidden_size, vocab_size)` to predict next word\n",
        "- Use `tensor.squeeze(1)` to remove batch dimension\n",
        "\n",
        "**What each function does:**\n",
        "| Function | Description |\n",
        "|----------|-------------|\n",
        "| `nn.Embedding(vocab_size, embed_size)` | Converts Hindi word indices to dense vectors |\n",
        "| `nn.GRU(embed_size, hidden_size)` | Processes current word + hidden state → next hidden state |\n",
        "| `nn.Linear(hidden_size, vocab_size)` | Fully connected layer that predicts probability for each word |\n",
        "| `output.squeeze(1)` | Removes dimension of size 1 (e.g., shape [1,256] → [256]) |\n",
        "\n",
        "**Documentation:**\n",
        "- [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) - Word embeddings\n",
        "- [nn.GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html) - Recurrent layer\n",
        "- [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) - Fully connected layer\n",
        "- [Tensor.squeeze](https://pytorch.org/docs/stable/generated/torch.squeeze.html) - Remove size-1 dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "116ae8ab",
      "metadata": {
        "id": "116ae8ab",
        "outputId": "5570e125-33d4-42b1-d703-05cabbe05e9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder defined\n"
          ]
        }
      ],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)  # Hint: Recurrent layer type\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)  # Hint: Fully connected layer\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        prediction = self.fc(output.squeeze(1))\n",
        "        return prediction, hidden\n",
        "\n",
        "print(\"Decoder defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd78cc4c",
      "metadata": {
        "id": "fd78cc4c"
      },
      "source": [
        "## 8. Prepare Training Data\n",
        "\n",
        "* **Convert to Tensors:** This cell converts all sentence pairs from text to tensor format for training.\n",
        "\n",
        "**Hints:**\n",
        "- Loop through all rows with `range(len(df))`\n",
        "- Use `sentence_to_indices()` for both English and Hindi\n",
        "- Store pairs as tuples in a list\n",
        "\n",
        "**What each function does:**\n",
        "| Function | Description |\n",
        "|----------|-------------|\n",
        "| `range(len(df))` | Creates sequence 0,1,2,...,n-1 for iterating through rows |\n",
        "| `sentence_to_indices(text, vocab)` | Our custom function - converts sentence to tensor of indices |\n",
        "| `list.append((a, b))` | Adds a tuple (English tensor, Hindi tensor) to the list |\n",
        "\n",
        "**Documentation:**\n",
        "- [range](https://docs.python.org/3/library/functions.html#func-range) - Generate number sequence\n",
        "- [list.append](https://docs.python.org/3/tutorial/datastructures.html) - Add item to list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ef7b78bb",
      "metadata": {
        "id": "ef7b78bb",
        "outputId": "a1a82283-0679-4cff-d324-198b1e4ac199",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 training pairs ready\n"
          ]
        }
      ],
      "source": [
        "# Convert all pairs to tensors\n",
        "training_pairs = []\n",
        "for idx in range(len(df_small)):  # Hint: Get number of rows\n",
        "    eng_tensor = sentence_to_indices(df_small['English'][idx], eng_word2idx)\n",
        "    hin_tensor = sentence_to_indices(df_small['Hindi'][idx], hin_word2idx)\n",
        "    training_pairs.append(    (eng_tensor, hin_tensor))  # Hint: Add item to list\n",
        "\n",
        "print(f\"{len(training_pairs)} training pairs ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddba224c",
      "metadata": {
        "id": "ddba224c"
      },
      "source": [
        "## 9. Create Models\n",
        "\n",
        "* **Initialize Models:** This cell creates encoder/decoder instances and sets up loss function and optimizers.\n",
        "\n",
        "**Hints:**\n",
        "- Set `EMBED_SIZE=128` and `HIDDEN_SIZE=256`\n",
        "- Use `nn.CrossEntropyLoss()` for classification loss\n",
        "- Use `torch.optim.Adam(model.parameters(), lr=0.001)` for optimization\n",
        "\n",
        "**What each function does:**\n",
        "| Function | Description |\n",
        "|----------|-------------|\n",
        "| `Encoder(vocab, embed, hidden)` | Creates encoder instance with specified sizes |\n",
        "| `Decoder(vocab, embed, hidden)` | Creates decoder instance with specified sizes |\n",
        "| `nn.CrossEntropyLoss()` | Loss function that measures how wrong predictions are (lower = better) |\n",
        "| `torch.optim.Adam(params, lr)` | Optimizer that updates weights to minimize loss; lr = learning rate |\n",
        "| `model.parameters()` | Returns all trainable weights in the model |\n",
        "\n",
        "**Documentation:**\n",
        "- [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) - Classification loss\n",
        "- [torch.optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) - Adam optimizer\n",
        "- [Module.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters) - Get model weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fc3e23b5",
      "metadata": {
        "id": "fc3e23b5",
        "outputId": "a170139c-9b6c-4b1f-caa1-2339482dffc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models ready!\n",
            "  Embedding: 128, Hidden: 256\n"
          ]
        }
      ],
      "source": [
        "# Model parameters\n",
        "EMBED_SIZE = 128\n",
        "HIDDEN_SIZE = 256\n",
        "\n",
        "# Create models\n",
        "encoder = Encoder(len(eng_word2idx), EMBED_SIZE, HIDDEN_SIZE)\n",
        "decoder = Decoder(len(hin_word2idx), EMBED_SIZE, HIDDEN_SIZE)\n",
        "\n",
        "# Setup training\n",
        "criterion = nn.CrossEntropyLoss()  # Hint: Loss function for classification\n",
        "enc_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.001)  # Hint: Adaptive optimizer\n",
        "dec_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001)  # Hint: Same optimizer\n",
        "\n",
        "print(f\"Models ready!\")\n",
        "print(f\"  Embedding: {EMBED_SIZE}, Hidden: {HIDDEN_SIZE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf604839",
      "metadata": {
        "id": "cf604839"
      },
      "source": [
        "## 10. Training Function\n",
        "\n",
        "* **Define Training Step:** This cell defines the function to train on one English-Hindi sentence pair.\n",
        "\n",
        "**Hints:**\n",
        "- Call `optimizer.zero_grad()` before forward pass\n",
        "- Use `tensor.unsqueeze(0)` to add batch dimension\n",
        "- Use `loss.backward()` to compute gradients\n",
        "- Call `optimizer.step()` to update weights\n",
        "\n",
        "**What each function does:**\n",
        "| Function | Description |\n",
        "|----------|-------------|\n",
        "| `optimizer.zero_grad()` | Clears old gradients - must do before each training step |\n",
        "| `tensor.unsqueeze(0)` | Adds a dimension at position 0 (e.g., [5] → [1,5] for batch) |\n",
        "| `encoder(input)` | Runs encoder forward pass, returns context vector |\n",
        "| `decoder(input, hidden)` | Runs decoder forward pass, returns (prediction, new_hidden) |\n",
        "| `criterion(pred, target)` | Calculates loss between prediction and actual target |\n",
        "| `loss.backward()` | Computes gradients via backpropagation |\n",
        "| `optimizer.step()` | Updates model weights using computed gradients |\n",
        "| `loss.item()` | Extracts Python number from single-element tensor |\n",
        "\n",
        "**Documentation:**\n",
        "- [Optimizer.zero_grad](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html) - Clear gradients\n",
        "- [Tensor.unsqueeze](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html) - Add dimension\n",
        "- [Tensor.backward](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html) - Backpropagation\n",
        "- [Optimizer.step](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html) - Update weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1e8a6aa8",
      "metadata": {
        "id": "1e8a6aa8",
        "outputId": "cf8cb5dd-36b4-46c4-80c9-0157ecef6d66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training function ready\n"
          ]
        }
      ],
      "source": [
        "def train_one_pair(eng_tensor, hin_tensor):\n",
        "    \"\"\"Train on one sentence pair\"\"\"\n",
        "    enc_optimizer.zero_grad()  # Hint: Clear old gradients\n",
        "    dec_optimizer.zero_grad()  # Hint: Clear old gradients\n",
        "\n",
        "    # Encode\n",
        "    context = encoder(eng_tensor.unsqueeze(0))\n",
        "\n",
        "    # Decode word by word\n",
        "    loss = 0\n",
        "    hidden = context\n",
        "    for i in range(hin_tensor.size(0)):\n",
        "        dec_input = torch.tensor([[1]]) if i == 0 else hin_tensor[i-1].unsqueeze(0).unsqueeze(0)\n",
        "        output, hidden = decoder(dec_input, hidden)\n",
        "        loss += criterion(output, hin_tensor[i].unsqueeze(0))\n",
        "\n",
        "    # Update weights\n",
        "    loss.backward()  # Hint: Compute gradients\n",
        "    enc_optimizer.step()  # Hint: Update weights\n",
        "    dec_optimizer.step()  # Hint: Update weights\n",
        "\n",
        "    return loss.item() / hin_tensor.size(0)\n",
        "\n",
        "print(\"Training function ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e78cd92a",
      "metadata": {
        "id": "e78cd92a"
      },
      "source": [
        "## 11. Train the Model\n",
        "\n",
        "* **Training Loop:** This cell runs the training loop for 100 epochs (~5-10 minutes).\n",
        "\n",
        "**Hints:**\n",
        "- Use `random.shuffle()` to randomize order each epoch\n",
        "- Use `time.time()` to track elapsed time\n",
        "- Print progress every 10 epochs with `epoch % 10 == 0`\n",
        "\n",
        "**What each function does:**\n",
        "| Function | Description |\n",
        "|----------|-------------|\n",
        "| `range(1, n+1)` | Creates sequence 1,2,3,...,n for epoch counting |\n",
        "| `random.shuffle(list)` | Randomly reorders list in-place (prevents memorizing order) |\n",
        "| `time.time()` | Returns current time in seconds (for measuring duration) |\n",
        "| `epoch % 10 == 0` | True every 10 epochs (modulo operator for progress printing) |\n",
        "\n",
        "**Documentation:**\n",
        "- [random.shuffle](https://docs.python.org/3/library/random.html#random.shuffle) - Randomize list order\n",
        "- [time.time](https://docs.python.org/3/library/time.html#time.time) - Current timestamp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2afcda07",
      "metadata": {
        "id": "2afcda07",
        "outputId": "d2d0f6f7-e65a-4496-96f2-eb9729af4560",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started...\n",
            "============================================================\n",
            "Epoch 10/100 | Loss: 0.212 | Time: 148.4s\n",
            "Epoch 20/100 | Loss: 0.165 | Time: 290.8s\n",
            "Epoch 30/100 | Loss: 0.148 | Time: 432.0s\n",
            "Epoch 40/100 | Loss: 0.135 | Time: 580.2s\n",
            "Epoch 50/100 | Loss: 0.129 | Time: 724.1s\n",
            "Epoch 60/100 | Loss: 0.124 | Time: 874.0s\n",
            "Epoch 70/100 | Loss: 0.123 | Time: 1028.7s\n",
            "Epoch 80/100 | Loss: 0.120 | Time: 1200.8s\n",
            "Epoch 90/100 | Loss: 0.120 | Time: 1354.8s\n",
            "Epoch 100/100 | Loss: 0.116 | Time: 1507.6s\n",
            "============================================================\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 100\n",
        "print(\"Training started...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "start_time = time.time()  # Hint: Get current timestamp\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    random.shuffle(training_pairs)  # Hint: Randomize list order\n",
        "    total_loss = 0\n",
        "\n",
        "    for eng_tensor, hin_tensor in training_pairs:\n",
        "        loss = train_one_pair(eng_tensor, hin_tensor)\n",
        "        total_loss += loss\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        avg_loss = total_loss / len(training_pairs)\n",
        "        print(f\"Epoch {epoch:2d}/{NUM_EPOCHS} | Loss: {avg_loss:.3f} | Time: {time.time()-start_time:.1f}s\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b6a5fcf",
      "metadata": {
        "id": "9b6a5fcf"
      },
      "source": [
        "## 12. Translation Function\n",
        "\n",
        "* **Define Inference:** This cell defines the function to translate English sentences to Hindi.\n",
        "\n",
        "**Hints:**\n",
        "- Use `torch.no_grad()` to disable gradient computation\n",
        "- Start decoder with `<SOS>` token (index 1)\n",
        "- Use `output.argmax(1)` to get predicted word index\n",
        "- Stop when `<EOS>` token (index 2) is generated\n",
        "- Use `' '.join()` to combine words\n",
        "\n",
        "**What each function does:**\n",
        "| Function | Description |\n",
        "|----------|-------------|\n",
        "| `torch.no_grad()` | Context manager that disables gradient tracking (faster, less memory) |\n",
        "| `encoder(input)` | Encodes English sentence into context vector |\n",
        "| `torch.tensor([[1]])` | Creates tensor with value 1 (SOS token) as decoder starting input |\n",
        "| `output.argmax(1)` | Returns index of highest probability word (the prediction) |\n",
        "| `tensor.item()` | Converts single-element tensor to Python int |\n",
        "| `dict.get(key, default)` | Looks up word for index, returns UNK if not found |\n",
        "| `' '.join(list)` | Combines list of words into single string with spaces |\n",
        "\n",
        "**Documentation:**\n",
        "- [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html) - Disable gradients\n",
        "- [Tensor.argmax](https://pytorch.org/docs/stable/generated/torch.argmax.html) - Index of max value\n",
        "- [Tensor.item](https://pytorch.org/docs/stable/generated/torch.Tensor.item.html) - Tensor to Python number\n",
        "- [str.join](https://docs.python.org/3/library/stdtypes.html#str.join) - Join strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "6918043c",
      "metadata": {
        "id": "6918043c",
        "outputId": "1fa87d0a-1fe7-4949-fbec-891dcf2d4d14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation function ready\n"
          ]
        }
      ],
      "source": [
        "def translate(sentence, max_len=20):\n",
        "    \"\"\"Translate English to Hindi\"\"\"\n",
        "    eng_tensor = sentence_to_indices(sentence, eng_word2idx)\n",
        "\n",
        "    with torch.no_grad():  # Hint: Disable gradient computation\n",
        "        context = encoder(eng_tensor.unsqueeze(0))\n",
        "\n",
        "    hindi_words = []\n",
        "    hidden = context\n",
        "    dec_input = torch.tensor([[1]])  # <SOS>\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():  # Hint: Same as above\n",
        "            output, hidden = decoder(dec_input, hidden)\n",
        "\n",
        "        predicted_idx = output.argmax(1).item()  # Hint: Get index of max value\n",
        "        if predicted_idx == 2:  # <EOS>\n",
        "            break\n",
        "\n",
        "        hindi_words.append(hin_idx2word.get(predicted_idx, '<UNK>'))\n",
        "        dec_input = torch.tensor([[predicted_idx]])\n",
        "\n",
        "    return ' '.join(hindi_words)  # Hint: Combine list into string\n",
        "\n",
        "print(\"Translation function ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ad5621f",
      "metadata": {
        "id": "1ad5621f"
      },
      "source": [
        "## 13. Test Translations\n",
        "\n",
        "* **Evaluate Model:** This cell tests the model on sample sentences from the training set.\n",
        "\n",
        "**Hints:**\n",
        "- Loop through specific indices `[0, 5, 10, 20, 30]`\n",
        "- Compare predicted vs actual translations\n",
        "- Use ✓ for match, ✗ for mismatch\n",
        "\n",
        "**What each function does:**\n",
        "| Function | Description |\n",
        "|----------|-------------|\n",
        "| `df['column'][idx]` | Gets value at specified index from DataFrame column |\n",
        "| `translate(sentence)` | Our custom function - translates English to Hindi |\n",
        "| `pred == actual` | String comparison - returns True if translations match exactly |\n",
        "\n",
        "**Documentation:**\n",
        "- [DataFrame indexing](https://pandas.pydata.org/docs/user_guide/indexing.html) - Access DataFrame values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "c48540ba",
      "metadata": {
        "id": "c48540ba",
        "outputId": "2ce94002-c49d-45e6-f36c-614a7213a1bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRANSLATION RESULTS\n",
            "======================================================================\n",
            "\n",
            "wrong English:   Help!\n",
            "  Predicted: <UNK>\n",
            "  Actual:    बचाओ!\n",
            "\n",
            "wrong English:   Hello!\n",
            "  Predicted: <UNK>\n",
            "  Actual:    नमस्कार।\n",
            "\n",
            "wrong English:   Awesome!\n",
            "  Predicted: <UNK>\n",
            "  Actual:    बहुत बढ़िया!\n",
            "\n",
            "wrong English:   Have fun.\n",
            "  Predicted: मज़े करना।\n",
            "  Actual:    मौज करना।\n",
            "\n",
            "correct English:   Excuse me.\n",
            "  Predicted: माफ़ कीजिए।\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"TRANSLATION RESULTS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for idx in [0, 5, 10, 20, 30]:\n",
        "    eng = df_small['English'][idx]  # Hint: Column name for English sentences\n",
        "    actual = df_small['Hindi'][idx]  # Hint: Column name for Hindi sentences\n",
        "    predicted = translate(eng)  # Hint: Function to translate\n",
        "\n",
        "    match = \"correct\" if predicted == actual else \"wrong\"\n",
        "    print(f\"\\n{match} English:   {eng}\")\n",
        "    print(f\"  Predicted: {predicted}\")\n",
        "    if predicted != actual:\n",
        "        print(f\"  Actual:    {actual}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa784c35",
      "metadata": {
        "id": "fa784c35"
      },
      "source": [
        "## 14. Try Your Own!\n",
        "\n",
        "* **Interactive Testing:** This cell lets you test the model on any English sentence.\n",
        "\n",
        "**Hints:**\n",
        "- Change the `your_sentence` variable to test different inputs\n",
        "- Model works best on short, simple sentences similar to training data\n",
        "\n",
        "**What each function does:**\n",
        "| Function | Description |\n",
        "|----------|-------------|\n",
        "| `translate(sentence)` | Takes English string, returns Hindi translation |\n",
        "| `print(f\"...\")` | Formatted string printing with variable interpolation |\n",
        "\n",
        "**Documentation:**\n",
        "- [f-strings](https://docs.python.org/3/tutorial/inputoutput.html#formatted-string-literals) - Formatted string literals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a397fc29",
      "metadata": {
        "id": "a397fc29",
        "outputId": "62296619-44f1-4a2d-832d-0de023266432",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: Hey need your help\n",
            "Hindi:   तुम <UNK> से शादी की।\n"
          ]
        }
      ],
      "source": [
        "# Change this sentence!\n",
        "your_sentence = \"Hey need your help\"\n",
        "\n",
        "translation = translate(your_sentence)  # Hint: Call the translation function\n",
        "print(f\"English: {your_sentence}\")\n",
        "print(f\"Hindi:   {translation}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "461bcd15",
      "metadata": {
        "id": "461bcd15"
      },
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "**What we built:**\n",
        "1. **Encoder**: Reads English → creates context\n",
        "2. **Decoder**: Uses context → generates Hindi\n",
        "\n",
        "**How it works:**\n",
        "```\n",
        "\"Hello\" → Encoder → [Context Vector] → Decoder → \"नमस्ते\"\n",
        "```\n",
        "\n",
        "**Key Components:**\n",
        "- Vocabulary: Words ↔ Numbers\n",
        "- Embeddings: Numbers → Vectors  \n",
        "- GRU: Process sequences\n",
        "- Training: Learn from examples\n",
        "\n",
        "**Congratulations!**  You built a translation system!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f75f5430",
      "metadata": {
        "id": "f75f5430"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}